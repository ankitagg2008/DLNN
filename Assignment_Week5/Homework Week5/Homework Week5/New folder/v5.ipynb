{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0663e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage import color\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier # using 1NN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm \n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927e50a",
   "metadata": {},
   "source": [
    "# 1. Download the [cow teat datasets](https://github.com/YoushanZhang/SCTL) (10 points) resize image to (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c76ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loader(train_dirs):\n",
    "    # Initialize empty lists for to store image arrays and labels\n",
    "    image_arrays = []\n",
    "    image_labels = []\n",
    "   \n",
    "    # loop to navitage all directories and read information from all the image files\n",
    "    for label, directory in enumerate(sorted(os.listdir(train_dirs))):\n",
    "        path = os.path.join(train_dirs, directory)\n",
    "\n",
    "        # check if it's a directory\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "\n",
    "        # Iterate over image files in the directory\n",
    "        for img_file in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "\n",
    "            # resize and store images\n",
    "            img = cv2.resize(plt.imread(img_path).copy(), (224, 224))\n",
    "            image_labels.append(label)\n",
    "            image_arrays.append(img)\n",
    "            \n",
    "    return image_arrays, image_labels\n",
    "\n",
    "\n",
    "def test_data_loader(data_dir):\n",
    "    # Get a list of image files in the test data directory\n",
    "    image_files = [f for f in os.listdir(data_dir) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Initialize lists for data and file names\n",
    "    image_arrays = []\n",
    "    image_names = []\n",
    "    \n",
    "    # Iterate over the image files\n",
    "    for file in image_files:\n",
    "        # Get the full path of the image\n",
    "        path = os.path.join(data_dir, file)\n",
    "        \n",
    "        # Read and preprocess the image\n",
    "        img = cv2.resize(plt.imread(path).copy(), (224, 224))\n",
    "        #img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Append the preprocessed image and file name to the lists\n",
    "        image_arrays.append(img)\n",
    "        image_names.append(file)\n",
    "    \n",
    "    # Convert lists to NumPy arrays and return\n",
    "    return image_arrays, image_names\n",
    "\n",
    "train_loader = r'C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Assignment_Week5\\Homework Week5\\Homework Week5\\Training'\n",
    "training_array, training_labels = train_data_loader(train_loader)\n",
    "training_array, training_labels = np.array(training_array), np.array(training_labels)\n",
    "\n",
    "test_loader=r'C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Assignment_Week5\\Homework Week5\\Homework Week5\\Test_Data'\n",
    "test_array, test_file_name = test_data_loader(test_loader)\n",
    "test_array, test_file_name = np.array(test_array), np.array(test_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c625d",
   "metadata": {},
   "source": [
    "### (1). Create a train data loader that returns image arrays and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b0b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loader(train_dirs):\n",
    "    # Initialize empty lists for to store image arrays and labels\n",
    "    image_arrays = []\n",
    "    image_labels = []\n",
    "   \n",
    "    # loop to navitage all directories and read information from all the image files\n",
    "    for label, directory in enumerate(sorted(os.listdir(train_dirs))):\n",
    "        path = os.path.join(train_dirs, directory)\n",
    "\n",
    "        # check if it's a directory\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "\n",
    "        # Iterate over image files in the directory\n",
    "        for img_file in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "\n",
    "            # resize and store images\n",
    "            img = cv2.resize(plt.imread(img_path).copy(), (224, 224))\n",
    "            image_labels.append(label)\n",
    "            image_arrays.append(img)\n",
    "            \n",
    "    return image_arrays, image_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c75bdb",
   "metadata": {},
   "source": [
    "### (2). Create a test data loader that returns image arrays and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541e9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_loader(data_dir):\n",
    "    # Get a list of image files in the test data directory\n",
    "    image_files = [f for f in os.listdir(data_dir) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Initialize lists for data and file names\n",
    "    image_arrays = []\n",
    "    image_names = []\n",
    "    \n",
    "    # Iterate over the image files\n",
    "    for file in image_files:\n",
    "        # Get the full path of the image\n",
    "        path = os.path.join(data_dir, file)\n",
    "        \n",
    "        # Read and preprocess the image\n",
    "        img = cv2.resize(plt.imread(path).copy(), (224, 224))\n",
    "        #img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Append the preprocessed image and file name to the lists\n",
    "        image_arrays.append(img)\n",
    "        image_names.append(file)\n",
    "    \n",
    "    # Convert lists to NumPy arrays and return\n",
    "    return image_arrays, image_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad569b2",
   "metadata": {},
   "source": [
    "### (3). Print image arrays, labels and file names dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09431a",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c273b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Array: (1149, 224, 224, 3)\n",
      "Training Label Shape: (1149,)\n"
     ]
    }
   ],
   "source": [
    "train_loader = r'C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Assignment_Week5\\Homework Week5\\Homework Week5\\Training'\n",
    "training_array, training_labels = train_data_loader(train_loader)\n",
    "training_array, training_labels = np.array(training_array), np.array(training_labels)\n",
    "\n",
    "print(\"Training Dataset Array:\",training_array.shape )\n",
    "print(\"Training Label Shape:\",training_labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8de9148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f15e42",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "   - The training dataset contains total 1149 images of size 224, 224 and has 3 channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b0d89",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6530b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Array: (380, 224, 224, 3)\n",
      "Test File Names Extracted: (380,)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_loader=r'C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Assignment_Week5\\Homework Week5\\Homework Week5\\Test_Data'\n",
    "test_array, test_file_name = test_data_loader(test_loader)\n",
    "test_array, test_file_name = np.array(test_array), np.array(test_file_name)\n",
    "\n",
    "print(\"Test Dataset Array:\",test_array.shape )\n",
    "print(\"Test File Names Extracted:\",test_file_name.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6afa108",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "   - The test dataset contains total 380 images of size 224, 224 and has 3 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44a4c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c63b53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedNeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 4)  # Replace 'num_classes' with the number of classes in your dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "223034b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your neural network (no arguments needed)\n",
    "model = ImprovedNeuralNetwork()\n",
    "\n",
    "\n",
    "# Define the optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e205d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.150682210922241\n",
      "Epoch [2/20], Loss: 509.5077819824219\n",
      "Epoch [3/20], Loss: 428.735107421875\n",
      "Epoch [4/20], Loss: 208.2771759033203\n",
      "Epoch [5/20], Loss: 79.84121704101562\n",
      "Epoch [6/20], Loss: 57.232791900634766\n",
      "Epoch [7/20], Loss: 25.68913459777832\n",
      "Epoch [8/20], Loss: 7.332444190979004\n",
      "Epoch [9/20], Loss: 5.7945098876953125\n",
      "Epoch [10/20], Loss: 3.50681209564209\n",
      "Epoch [11/20], Loss: 2.011784791946411\n",
      "Epoch [12/20], Loss: 1.6694309711456299\n",
      "Epoch [13/20], Loss: 1.2791826725006104\n",
      "Epoch [14/20], Loss: 1.2072120904922485\n",
      "Epoch [15/20], Loss: 1.2249876260757446\n",
      "Epoch [16/20], Loss: 1.2202109098434448\n",
      "Epoch [17/20], Loss: 1.151757001876831\n",
      "Epoch [18/20], Loss: 1.0337350368499756\n",
      "Epoch [19/20], Loss: 1.0644659996032715\n",
      "Epoch [20/20], Loss: 0.9913707375526428\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have loaded your training data into 'training_array' and 'training_labels'\n",
    "\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "training_data = torch.Tensor(training_array)\n",
    "training_labels = torch.LongTensor(training_labels)\n",
    "\n",
    "# Define the number of classes (replace with the actual number)\n",
    "num_classes = 10  # Change to your actual number of classes\n",
    "\n",
    "\n",
    "# # Set the device (CPU or GPU)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# print(device)\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "\n",
    "# Assuming 'training_data' has shape [1149, 224, 224, 3]\n",
    "# You can permute the dimensions to match the expected shape [1149, 3, 224, 224]\n",
    "training_data = training_data.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # You can adjust this as needed\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(training_data.to(device))\n",
    "    loss = criterion(outputs, training_labels.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'trained_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6d3d564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to dlnn_output.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define your neural network class\n",
    "\n",
    "# Define your data loading functions\n",
    "\n",
    "# Load your trained model\n",
    "model = ImprovedNeuralNetwork()\n",
    "model.load_state_dict(torch.load('trained_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Assuming 'test_array' has shape [num_samples, 224, 224, 3]\n",
    "test_data = torch.Tensor(test_array).permute(0, 3, 1, 2)\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_data)\n",
    "\n",
    "# Get the predicted labels (assuming you are using softmax as the final layer)\n",
    "_, predicted_labels = torch.max(test_outputs, 1)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({\n",
    "    'test_file_names': test_file_name,\n",
    "    'predicted_labels': predicted_labels\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('dlnn_output.csv', index=False)\n",
    "\n",
    "print(\"Results saved to dlnn_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336327dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
