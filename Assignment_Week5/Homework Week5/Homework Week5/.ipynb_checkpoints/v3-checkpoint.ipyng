{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0663e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage import color\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier # using 1NN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm \n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927e50a",
   "metadata": {},
   "source": [
    "# 1. Download the [cow teat datasets](https://github.com/YoushanZhang/SCTL) (10 points) resize image to (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c625d",
   "metadata": {},
   "source": [
    "### (1). Create a train data loader that returns image arrays and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b0b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loader(train_dirs):\n",
    "    # Initialize empty lists for to store image arrays and labels\n",
    "    image_arrays = []\n",
    "    image_labels = []\n",
    "   \n",
    "    # loop to navitage all directories and read information from all the image files\n",
    "    for label, directory in enumerate(sorted(os.listdir(train_dirs))):\n",
    "        path = os.path.join(train_dirs, directory)\n",
    "\n",
    "        # check if it's a directory\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "\n",
    "        # Iterate over image files in the directory\n",
    "        for img_file in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "\n",
    "            # resize and store images\n",
    "            img = cv2.resize(plt.imread(img_path).copy(), (224, 224))\n",
    "            image_labels.append(label)\n",
    "            image_arrays.append(img)\n",
    "            \n",
    "    return image_arrays, image_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c75bdb",
   "metadata": {},
   "source": [
    "### (2). Create a test data loader that returns image arrays and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541e9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_loader(data_dir):\n",
    "    # Get a list of image files in the test data directory\n",
    "    image_files = [f for f in os.listdir(data_dir) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Initialize lists for data and file names\n",
    "    image_arrays = []\n",
    "    image_names = []\n",
    "    \n",
    "    # Iterate over the image files\n",
    "    for file in image_files:\n",
    "        # Get the full path of the image\n",
    "        path = os.path.join(data_dir, file)\n",
    "        \n",
    "        # Read and preprocess the image\n",
    "        img = cv2.resize(plt.imread(path).copy(), (224, 224))\n",
    "        #img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Append the preprocessed image and file name to the lists\n",
    "        image_arrays.append(img)\n",
    "        image_names.append(file)\n",
    "    \n",
    "    # Convert lists to NumPy arrays and return\n",
    "    return image_arrays, image_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad569b2",
   "metadata": {},
   "source": [
    "### (3). Print image arrays, labels and file names dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09431a",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c273b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Array: (1149, 224, 224, 3)\n",
      "Training Label Shape: (1149,)\n"
     ]
    }
   ],
   "source": [
    "train_loader = r'C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Assignment_Week5\\Homework Week5\\Homework Week5\\Training'\n",
    "training_array, training_labels = train_data_loader(train_loader)\n",
    "training_array, training_labels = np.array(training_array), np.array(training_labels)\n",
    "\n",
    "print(\"Training Dataset Array:\",training_array.shape )\n",
    "print(\"Training Label Shape:\",training_labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f245428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f15e42",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "   - The training dataset contains total 1149 images of size 224, 224 and has 3 channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b0d89",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6530b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Array: (380, 224, 224, 3)\n",
      "Test File Names Extracted: (380,)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_loader=r'C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Assignment_Week5\\Homework Week5\\Homework Week5\\Test_Data'\n",
    "test_array, test_file_name = test_data_loader(test_loader)\n",
    "test_array, test_file_name = np.array(test_array), np.array(test_file_name)\n",
    "\n",
    "print(\"Test Dataset Array:\",test_array.shape )\n",
    "print(\"Test File Names Extracted:\",test_file_name.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6afa108",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "   - The test dataset contains total 380 images of size 224, 224 and has 3 channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5ec7c",
   "metadata": {},
   "source": [
    "# 2. Extract features of training and test images using HOG (20 points)\n",
    "Please print the size of extracted features, e.g., training features: 1149 * d, test features: 380 *d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ac0b3",
   "metadata": {},
   "source": [
    "#### Extracting HOG Features for Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd8f13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset HOG features: 1149 * 11664\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store features and labels\n",
    "hog_features = []\n",
    "hog_labels = []\n",
    "\n",
    "# Extract features for training images\n",
    "for i, img in enumerate(training_array):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "    # Compute HOG descriptors for each grayscale image\n",
    "    features = hog(img_gray, pixels_per_cell=(16, 16), transform_sqrt=False)\n",
    "    hog_features.append(features)\n",
    "    hog_labels.append(training_labels[i])\n",
    "    \n",
    "# Convert feature lists to numpy arrays\n",
    "hog_features = np.array(hog_features)\n",
    "hog_labels = np.array(hog_labels)\n",
    "\n",
    "# Print the size of extracted features\n",
    "print(f\"Training Dataset HOG features: {hog_features.shape[0]} * {hog_features.shape[1]}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463dfd18",
   "metadata": {},
   "source": [
    "#### Extracting HOG Features for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eacabc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset HOG features: 380 * 11664\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store features and labels\n",
    "hog_features_test = []\n",
    "hog_labels_test = []\n",
    "\n",
    "# Extract features for test images\n",
    "for i, img in enumerate(test_array):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Compute HOG descriptors for each grayscale image\n",
    "    features = hog(img_gray, pixels_per_cell=(16, 16), transform_sqrt=True)\n",
    "    \n",
    "    hog_features_test.append(features)\n",
    "    hog_labels_test.append(test_file_name[i])\n",
    "\n",
    "# Convert feature lists to numpy arrays\n",
    "hog_features_test = np.array(hog_features_test)\n",
    "hog_labels_test = np.array(hog_labels_test)\n",
    "\n",
    "# Print the size of extracted features\n",
    "print(f\"Test Dataset HOG features: {hog_features_test.shape[0]} * {hog_features_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5339f5",
   "metadata": {},
   "source": [
    "# 3. Extract features of training and test images using SIFT (20 points)\n",
    "Please print the size of extracted features, e.g., training features: 1149 * d, test features: 380 *d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e9f0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT(image):\n",
    "    sift = cv2.xfeatures2d.SIFT_create(50)\n",
    "    keypoints, descriptors = sift.detectAndCompute(image,None)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b7b94",
   "metadata": {},
   "source": [
    "#### Extracting SIFT Features for Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c0348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: 1149 * 128\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store SIFT features\n",
    "sift_features = []\n",
    "\n",
    "# Load the training images\n",
    "for img in training_array:\n",
    "    gray1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    descriptors = SIFT(gray1)\n",
    "    if descriptors is not None:\n",
    "        sift_features.append(descriptors.flatten()[:128])\n",
    "    else:\n",
    "        sift_features.append(np.array([0]*(128)).flatten())\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "sift_features = np.array(sift_features)\n",
    "\n",
    "# Print the size of the extracted features\n",
    "print(f\"Training features: {sift_features.shape[0]} * {sift_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b93fc8c",
   "metadata": {},
   "source": [
    "#### Extracting HOG Features for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f00bf390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features: 380 * 128\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store SIFT features\n",
    "sift_features_test = []\n",
    "\n",
    "# Load the test images\n",
    "for img in test_array:\n",
    "    gray1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    descriptors = SIFT(gray1)\n",
    "    if descriptors is not None:\n",
    "        sift_features_test.append(descriptors.flatten()[:128])\n",
    "    else:\n",
    "        sift_features_test.append(np.array([0]*(128)).flatten())\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "sift_features_test = np.array(sift_features_test)\n",
    "\n",
    "# Print the size of the extracted features\n",
    "print(f\"Test features: {sift_features_test.shape[0]} * {sift_features_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753188e2",
   "metadata": {},
   "source": [
    "# 4. Extract features of training and test images using SURF (20 points)\n",
    "Please print the size of extracted features, e.g., training features: 1149 * d, test features: 380 *d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c85f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract SURF features\n",
    "def SURF(image):\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    keypoints, descriptors = surf.detectAndCompute(image, None)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ca78e",
   "metadata": {},
   "source": [
    "#### Extracting SURF Features for Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5abd5f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: 1149 * 128\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store SURF features\n",
    "surf_features = []\n",
    "\n",
    "# Load the training images\n",
    "for img in training_array:\n",
    "    descriptors = SURF(img)\n",
    "    if descriptors is not None:\n",
    "        surf_features.append(descriptors.flatten()[:128])\n",
    "    else:\n",
    "        surf_features.append(np.zeros(128, dtype=np.float32))\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "surf_features = np.array(surf_features, dtype=np.float32)\n",
    "\n",
    "# Print the size of the extracted features\n",
    "print(f\"Training features: {surf_features.shape[0]} * {surf_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2718e",
   "metadata": {},
   "source": [
    "#### Extracting SURF Features for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d5562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features: 380 * 128\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store SURF features\n",
    "surf_features_test = []\n",
    "\n",
    "# Load the test images\n",
    "for img in test_array:\n",
    "    descriptors = SURF(img)\n",
    "    if descriptors is not None:\n",
    "        surf_features_test.append(descriptors.flatten()[:128])\n",
    "    else:\n",
    "        surf_features_test.append(np.zeros(128, dtype=np.float32))\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "surf_features_test = np.array(surf_features_test, dtype=np.float32)\n",
    "\n",
    "# Print the size of the extracted features\n",
    "print(f\"Test features: {surf_features_test.shape[0]} * {surf_features_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3e068",
   "metadata": {},
   "source": [
    "# 5. Call SVM and kNN from scikit-learn and train the extracted HOG, SIFT and SURF features, respectively, save three CSV files of test dataset using three features (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea49db",
   "metadata": {},
   "source": [
    "### (1). SVM and KNN using HOG features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87020bcf",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db28dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(hog_features, hog_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a68355",
   "metadata": {},
   "source": [
    "#### Training and Predicting SVM Classifier and Saving result to a CSV File. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4172ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM HOG predictions saved to CSV files\n"
     ]
    }
   ],
   "source": [
    "# Train an SVM classifier\n",
    "svm_clf = SVC(kernel='rbf')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Finally, make predictions on the test set\n",
    "test_predictions = svm_clf.predict(hog_features_test)\n",
    "\n",
    "svm_hog_predictions = pd.DataFrame({'a':test_file_name, 'b':test_predictions})\n",
    "\n",
    "svm_hog_predictions.to_csv(r\"C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Output\\svm_hog_predictions_svm.csv\", index=False, header=False)\n",
    "print(\"SVM HOG predictions saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733843cb",
   "metadata": {},
   "source": [
    "#### Training and Predicting KNN Classifier and Saving result to a CSV File. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b302dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN HOG predictions saved to CSV files\n"
     ]
    }
   ],
   "source": [
    "# Train an KNN classifier\n",
    "n=20\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=n)\n",
    "knn_clf.fit(hog_features, hog_labels)\n",
    "\n",
    "# Finally, make predictions on the test set\n",
    "test_predictions = knn_clf.predict(hog_features_test)\n",
    "\n",
    "knn_hog_predictions = pd.DataFrame({'a':test_file_name, 'b':test_predictions})\n",
    "\n",
    "knn_hog_predictions.to_csv(r\"C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Output\\knn_hog_predictions_svm.csv\", index=False, header=False)\n",
    "print(\"KNN HOG predictions saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163e551",
   "metadata": {},
   "source": [
    "### (2). SVM and KNN using SIFT features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5b7c4",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b71d1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(sift_features, training_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2378e",
   "metadata": {},
   "source": [
    "#### Training and Predicting SVM Classifier and Saving result to a CSV File. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8a7fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM SIFT predictions saved to CSV files\n"
     ]
    }
   ],
   "source": [
    "# Create and Train an SVM classifier\n",
    "svm_clf = SVC(kernel='rbf', C=0.5, gamma='scale', max_iter=-1, degree=5)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test setset images\n",
    "test_predictions = svm_clf.predict(sift_features_test)\n",
    "\n",
    "#Converting to dataframe to be write to csvfile\n",
    "svm_sift_predictions = pd.DataFrame({'Filename':test_file_name, 'Prediction':test_predictions})\n",
    "\n",
    "svm_sift_predictions.to_csv(r\"C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Output\\svm_sift_predictions_svm.csv\", index=False, header=False)\n",
    "print(\"SVM SIFT predictions saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42796f",
   "metadata": {},
   "source": [
    "#### Training and Predicting KNN Classifier and Saving result to a CSV File. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb9c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN SIFT predictions saved to CSV files\n"
     ]
    }
   ],
   "source": [
    "# Create and Train a KNN classifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=20, weights ='distance')\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test setset images\n",
    "test_predictions = knn_clf.predict(sift_features_test)\n",
    "\n",
    "knn_sift_predictions = pd.DataFrame({'a':test_file_name, 'b':test_predictions})\n",
    "\n",
    "knn_sift_predictions.to_csv(r\"C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Output\\knn_sift_predictions_svm.csv\", index=False, header=False)\n",
    "print(\"KNN SIFT predictions saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908c75a",
   "metadata": {},
   "source": [
    "### (3). SVM and KNN using SURF features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ad5ed",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ff087f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(surf_features, training_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c0d6f",
   "metadata": {},
   "source": [
    "#### Training and Predicting SVM Classifier and Saving result to a CSV File. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c30cd97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM SURF predictions saved to CSV files\n"
     ]
    }
   ],
   "source": [
    "# Create and Train an SVM classifier\n",
    "svm_clf = SVC(kernel='rbf', C=0.5, gamma='scale', max_iter=-1, degree=5)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test setset images\n",
    "test_predictions = svm_clf.predict(surf_features_test)\n",
    "\n",
    "#Converting to dataframe to be write to csvfile\n",
    "svm_surf_predictions = pd.DataFrame({'Filename':test_file_name, 'Prediction':test_predictions})\n",
    "\n",
    "svm_surf_predictions.to_csv(r\"C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Output\\svm_surf_predictions_svm.csv\", index=False, header=False)\n",
    "print(\"SVM SURF predictions saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6de38",
   "metadata": {},
   "source": [
    "#### Training and Predicting KNN Classifier and Saving result to a CSV File. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "981dae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN SIFT predictions saved to CSV files\n"
     ]
    }
   ],
   "source": [
    "# Create and Train a KNN classifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=20, weights ='distance')\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test setset images\n",
    "test_predictions = knn_clf.predict(surf_features_test)\n",
    "\n",
    "knn_surf_predictions = pd.DataFrame({'a':test_file_name, 'b':test_predictions})\n",
    "\n",
    "knn_surf_predictions.to_csv(r\"C:\\AKA\\Backup Dell Laptop\\D Drive\\YU\\Semester 2\\Neural Network\\DLNN\\Output\\knn_surf_predictions_svm.csv\", index=False, header=False)\n",
    "print(\"KNN SIFT predictions saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd101cf",
   "metadata": {},
   "source": [
    "# 6. Report the accuracy using Cow_teat_classfication_accuracy software, please attach the results image here (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411fbc7",
   "metadata": {},
   "source": [
    "#### SVM Accuracy with HOG Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa24c2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be7ac2be",
   "metadata": {},
   "source": [
    "#### KNN Accuracy with HOG Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c790409",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2fee072",
   "metadata": {},
   "source": [
    "#### SVM Accuracy with SIFT Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7215fe57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b110bf85",
   "metadata": {},
   "source": [
    "#### KNN Accuracy with SIFT Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa24c2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4bcbf44",
   "metadata": {},
   "source": [
    "#### SVM Accuracy with Surf Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa24c2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "708a13bd",
   "metadata": {},
   "source": [
    "#### KNN Accuracy with Surf Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa24c2c",
   "metadata": {},
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf698180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d78b7ccd",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2019/10/detailed-guide-powerful-sift-technique-image-matching-python/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
