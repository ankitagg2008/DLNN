{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0663e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage import color\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier # using 1NN\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927e50a",
   "metadata": {},
   "source": [
    "# 1. Download the [cow teat datasets](https://github.com/YoushanZhang/SCTL) (10 points) resize image to (224, 224)\n",
    "\n",
    "### (1). Create a train data loader that returns image arrays and labels\n",
    "### (2). Create a test data loader that returns image arrays and file names\n",
    "### (3). Print image arrays, labels and file names dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b054f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train data loader\n",
    "def custom_train_loader(train_dirs):\n",
    "    image_arrays = []\n",
    "    image_labels = []\n",
    "\n",
    "    for label, directory in enumerate(sorted(os.listdir(train_dirs))):\n",
    "        path = os.path.join(train_dirs, directory)\n",
    "\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "        for img_file in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            img = cv2.resize(plt.imread(img_path).copy(), (224, 224))\n",
    "            image_labels.append(label)\n",
    "            image_arrays.append(img)\n",
    "    return image_arrays, image_labels\n",
    "\n",
    "#define test data loader\n",
    "def custom_test_loader(test_dir):\n",
    "    image_arrays = []\n",
    "    file_names = []\n",
    "    for img_file in sorted(os.listdir(test_dir)):\n",
    "        img_path = os.path.join(test_dir, img_file)\n",
    "\n",
    "        if os.path.isfile(img_path) and img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "            img = cv2.resize(cv2.imread(img_path), (224, 224))\n",
    "            image_arrays.append(img)\n",
    "            file_names.append(img_file)\n",
    "\n",
    "    return image_arrays, file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed81d0f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/Users/ling/Documents/course/DL/W5/Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-94f7094f8ca2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print image arrays, labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'/Users/ling/Documents/course/DL/W5/Train'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_train_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-197be18b9f52>\u001b[0m in \u001b[0;36mcustom_train_loader\u001b[1;34m(train_dirs)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mimage_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dirs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dirs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/Users/ling/Documents/course/DL/W5/Train'"
     ]
    }
   ],
   "source": [
    "# Print image arrays, labels\n",
    "train_dir = r'/Users/ling/Documents/course/DL/W5/Train'\n",
    "train_array, train_labels = custom_train_loader(train_dir)\n",
    "train_array, train_labels = np.array(training_array), np.array(training_labels)\n",
    "\n",
    "print(\"Training Dataset Array:\",train_array.shape )\n",
    "print(\"Training Label Shape:\",train_labels.shape )\n",
    "\n",
    "#Print image arrays ,file names\n",
    "test_dir = r'/Users/ling/Documents/course/DL/W5/Test'\n",
    "test_array, test_filename = custom_test_loader(test_dir)\n",
    "test_array, test_filename = np.array(test_array), np.array(test_filename)\n",
    "\n",
    "print(\"Test Dataset Array:\",test_array.shape )\n",
    "print(\"Test filename Shape:\",test_filename.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5ec7c",
   "metadata": {},
   "source": [
    "# 2. Extract features of training and test images using HOG (20 points)\n",
    "Please print the size of extracted features, e.g., training features: 1149 * d, test features: 380 *d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea4ac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0494fba80d5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# 提取训练集和测试集特征\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mtrain_hog_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_hog_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mtest_hog_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_hog_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_array' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 定义HOG参数\n",
    "orientations = 9\n",
    "pixels_per_cell = (8, 8)\n",
    "cells_per_block = (2, 2)\n",
    "print(1)\n",
    "\n",
    "# 提取训练集图像特征\n",
    "def extract_hog_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        # 将图像转换为灰度图\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # 计算HOG特征\n",
    "        fd, hog_image = hog(gray, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block, visualize=True)\n",
    "        # 使用直方图均衡化增强图像\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "        features.append(fd)\n",
    "    return features\n",
    "\n",
    "# 提取训练集和测试集特征\n",
    "train_hog_features = extract_hog_features(train_array)\n",
    "test_hog_features = extract_hog_features(test_array)\n",
    "\n",
    "# 打印特征的大小\n",
    "print(\"Training features:\", len(train_hog_features), \"*\", len(train_hog_features[0]))\n",
    "print(\"Test features:\", len(test_hog_features), \"*\", len(test_hog_features[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5339f5",
   "metadata": {},
   "source": [
    "# 3. Extract features of training and test images using SIFT (20 points)\n",
    "Please print the size of extracted features, e.g., training features: 1149 * d, test features: 380 *d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9f0084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: 1149 * 37\n",
      "Test features: 380 * 58\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 提取训练集图像特征\n",
    "def extract_sift_features(images):\n",
    "    sift = cv2.SIFT_create()\n",
    "    features = []\n",
    "    for img in images:\n",
    "        # 将图像转换为灰度图\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # 检测SIFT关键点和描述符\n",
    "        keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "        # 如果有关键点，将描述符加入特征列表\n",
    "        features.append(descriptors)\n",
    "#         if descriptors is not None:\n",
    "#             features.append(descriptors)\n",
    "    return features\n",
    "\n",
    "# 提取训练集和测试集特征\n",
    "train_sift_features = extract_sift_features(train_array)\n",
    "test_sift_features = extract_sift_features(test_array)\n",
    "\n",
    "# 打印特征的大小\n",
    "print(\"Training features:\", len(train_sift_features), \"*\", len(train_sift_features[0]))\n",
    "print(\"Test features:\", len(test_sift_features), \"*\", len(test_sift_features[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753188e2",
   "metadata": {},
   "source": [
    "# 4. Extract features of training and test images using SURF (20 points)\n",
    "Please print the size of extracted features, e.g., training features: 1149 * d, test features: 380 *d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a554f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: 1149 * 44\n",
      "Test features: 380 * 175\n"
     ]
    }
   ],
   "source": [
    "# 提取ORB特征\n",
    "def extract_orb_features(images):\n",
    "    orb = cv2.ORB_create()\n",
    "    features = []\n",
    "    for img in images:\n",
    "        # 将图像转换为灰度图\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # 检测ORB关键点和描述符\n",
    "        keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
    "        # 如果有关键点和描述符，将描述符加入特征列表\n",
    "        features.append(descriptors)\n",
    "#         if descriptors is not None:\n",
    "#             features.append(descriptors)\n",
    "    return features\n",
    "\n",
    "# 提取训练集和测试集特征\n",
    "train_surf_features = extract_orb_features(train_array)\n",
    "test_surf_features = extract_orb_features(test_array)\n",
    "\n",
    "# 打印特征的大小\n",
    "print(\"Training features:\", len(train_surf_features), \"*\", len(train_surf_features[0]) if len(train_surf_features) > 0 else 0)\n",
    "print(\"Test features:\", len(test_surf_features), \"*\", len(test_surf_features[0]) if len(test_surf_features) > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3e068",
   "metadata": {},
   "source": [
    "# 5. Call SVM and kNN from scikit-learn and train the extracted HOG, SIFT and SURF features, respectively, save three CSV files of test dataset using three features (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68704a6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1149,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# SVM classifier with SIFT features\u001b[39;00m\n\u001b[1;32m     33\u001b[0m svm_sift_classifier \u001b[38;5;241m=\u001b[39m SVC()\n\u001b[0;32m---> 34\u001b[0m \u001b[43msvm_sift_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sift_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m svm_sift_predictions \u001b[38;5;241m=\u001b[39m svm_sift_classifier\u001b[38;5;241m.\u001b[39mpredict(test_sift_features)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# kNN classifier with SIFT features\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1149,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming you have loaded your training and test datasets and extracted features\n",
    "# For example:\n",
    "# train_hog_features, train_labels = extract_hog_features(train_images)\n",
    "# test_hog_features = extract_hog_features(test_images)\n",
    "\n",
    "# train_sift_features, train_labels = extract_sift_features(train_images)\n",
    "# test_sift_features = extract_sift_features(test_images)\n",
    "\n",
    "# train_surf_features, train_labels = extract_surf_features(train_images)\n",
    "# test_surf_features = extract_surf_features(test_images)\n",
    "\n",
    "# SVM classifier with HOG features\n",
    "svm_hog_classifier = SVC()\n",
    "svm_hog_classifier.fit(train_hog_features, train_labels)\n",
    "svm_hog_predictions = svm_hog_classifier.predict(test_hog_features)\n",
    "\n",
    "# kNN classifier with HOG features\n",
    "knn_hog_classifier = KNeighborsClassifier()\n",
    "knn_hog_classifier.fit(train_hog_features, train_labels)\n",
    "knn_hog_predictions = knn_hog_classifier.predict(test_hog_features)\n",
    "\n",
    "# SVM classifier with SIFT features\n",
    "svm_sift_classifier = SVC()\n",
    "svm_sift_classifier.fit(train_sift_features, train_labels)\n",
    "svm_sift_predictions = svm_sift_classifier.predict(test_sift_features)\n",
    "\n",
    "# kNN classifier with SIFT features\n",
    "knn_sift_classifier = KNeighborsClassifier()\n",
    "knn_sift_classifier.fit(train_sift_features, train_labels)\n",
    "knn_sift_predictions = knn_sift_classifier.predict(test_sift_features)\n",
    "\n",
    "# SVM classifier with SURF features\n",
    "svm_surf_classifier = SVC()\n",
    "svm_surf_classifier.fit(train_surf_features, train_labels)\n",
    "svm_surf_predictions = svm_surf_classifier.predict(test_surf_features)\n",
    "\n",
    "# kNN classifier with SURF features\n",
    "knn_surf_classifier = KNeighborsClassifier()\n",
    "knn_surf_classifier.fit(train_surf_features, train_labels)\n",
    "knn_surf_predictions = knn_surf_classifier.predict(test_surf_features)\n",
    "\n",
    "# Save predictions to CSV files\n",
    "svm_hog_df = pd.DataFrame({'File Name': test_filename, 'Predicted Label': svm_hog_predictions})\n",
    "svm_hog_df.to_csv('svm_hog_predictions.csv', index=False)\n",
    "\n",
    "knn_hog_df = pd.DataFrame({'File Name': test_filename, 'Predicted Label': knn_hog_predictions})\n",
    "knn_hog_df.to_csv('knn_hog_predictions.csv', index=False)\n",
    "\n",
    "svm_sift_df = pd.DataFrame({'File Name': test_filename, 'Predicted Label': svm_sift_predictions})\n",
    "svm_sift_df.to_csv('svm_sift_predictions.csv', index=False)\n",
    "\n",
    "knn_sift_df = pd.DataFrame({'File Name': test_filename, 'Predicted Label': knn_sift_predictions})\n",
    "knn_sift_df.to_csv('knn_sift_predictions.csv', index=False)\n",
    "\n",
    "svm_surf_df = pd.DataFrame({'File Name': test_filename, 'Predicted Label': svm_surf_predictions})\n",
    "svm_surf_df.to_csv('svm_surf_predictions.csv', index=False)\n",
    "\n",
    "knn_surf_df = pd.DataFrame({'File Name': test_filename, 'Predicted Label': knn_surf_predictions})\n",
    "knn_surf_df.to_csv('knn_surf_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd101cf",
   "metadata": {},
   "source": [
    "# 6. Report the accuracy using Cow_teat_classfication_accuracy software, please attach the results image here (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea49db",
   "metadata": {},
   "source": [
    "### (1). SVM and 1NN using HOG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecc816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1163e551",
   "metadata": {},
   "source": [
    "### (2). SVM and 1NN using SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a54656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2908c75a",
   "metadata": {},
   "source": [
    "### (3). SVM and 1NN using SURF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fb829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
